# Env
# Recommended for development and troubleshooting
# DEBUG=True
# APP_ENV=development
ENVIRONMENT='dev'

# Database
DATABASE_TYPE='postgresql'
DATABASE_HOST='127.0.0.1'
DATABASE_PORT=5432
DATABASE_USER='postgres'
DATABASE_PASSWORD='123456'
DATABASE_SCHEMA='postgres'  # Supabase uses 'postgres' not 'agents_backend'

# Redis
REDIS_HOST='127.0.0.1'
REDIS_PORT=6379
REDIS_PASSWORD=''
REDIS_USERNAME='default'  # For Redis 6+ ACL (cloud Redis). Use 'default' if no specific user.
REDIS_DATABASE=0

# Token
TOKEN_SECRET_KEY='1VkVF75nsNABBjK_7-qz7GtzNy3AMvktc9TCPwKczCk'
# Opera Log
OPERA_LOG_ENCRYPT_SECRET_KEY='d77b25790a804c2b4a339dd0207941e4cefa5751935a33735bc73bb7071a005b'
# [ App ] task
# Celery
CELERY_BROKER_REDIS_DATABASE=1
# Rabbitmq
CELERY_RABBITMQ_HOST='127.0.0.1'
CELERY_RABBITMQ_PORT=5672
CELERY_RABBITMQ_USERNAME='guest'
CELERY_RABBITMQ_PASSWORD='guest'
# [ Plugin ] oauth2
OAUTH2_GITHUB_CLIENT_ID='test'
OAUTH2_GITHUB_CLIENT_SECRET='test'
OAUTH2_GOOGLE_CLIENT_ID='test'
OAUTH2_GOOGLE_CLIENT_SECRET='test'
OAUTH2_LINUX_DO_CLIENT_ID='test'
OAUTH2_LINUX_DO_CLIENT_SECRET='test'
# [ Plugin ] email
EMAIL_USERNAME=''
EMAIL_PASSWORD=''

EMAIL_HOST=smtp.gmail.com
EMAIL_PORT=587
EMAIL_USER=your@email.com
EMAIL_PASSWORD=your_app_password
EMAIL_USE_TLS=True
EMAIL_FROM=noreply@example.com

EMAIL_CAPTCHA_REDIS_PREFIX=agents_backend:email:captcha
EMAIL_CAPTCHA_EXPIRE_SECONDS=300


# ============================================================
# [ Module ] Agent - LangChain/LangGraph AI Agents
# ============================================================

# --------------------------------------------------------------------------
# [LLM Provider Configuration]
# --------------------------------------------------------------------------
# Select your LLM provider and configure its settings.
# The selected provider will be used for ALL agents in the system.
#
# Supported providers:
#   - openai        : OpenAI GPT models (langchain-openai)
#   - anthropic     : Anthropic Claude models (langchain-anthropic)
#   - gemini        : Google Gemini models (langchain-google-genai)
#   - deepseek      : DeepSeek models (langchain-deepseek)
#   - groq          : Groq-hosted models (langchain-groq)
#   - huggingface   : HuggingFace models (langchain-huggingface)
#   - ollama        : Local Ollama models (langchain-ollama)
#   - openai_compat : Custom OpenAI-compatible APIs (langchain-openai with base_url)
# --------------------------------------------------------------------------

# Select LLM Provider (required)
LLM_PROVIDER="openai"

# Common LLM Settings (applies to all providers)
LLM_MAX_RETRIES=3
LLM_TOKEN_LIMIT=200000
LLM_TEMPERATURE=0.7

# --------------------------------------------------------------------------
# OpenAI Configuration (provider: openai)
# Package: pip install langchain-openai
# --------------------------------------------------------------------------
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-5"
OPENAI_BASE_URL=""  # Optional: Custom base URL for proxies/emulators

# --------------------------------------------------------------------------
# Anthropic Configuration (provider: anthropic)
# Package: pip install langchain-anthropic
# --------------------------------------------------------------------------
ANTHROPIC_API_KEY=""
ANTHROPIC_MODEL="claude-sonnet-4-20250514"

# --------------------------------------------------------------------------
# Google Gemini Configuration (provider: gemini)
# Package: pip install langchain-google-genai
# --------------------------------------------------------------------------
GOOGLE_API_KEY=""
GEMINI_MODEL="gemini-2.0-flash"
# Optional: Vertex AI Configuration
# GOOGLE_GENAI_USE_VERTEXAI=false
# GOOGLE_CLOUD_PROJECT=""

# --------------------------------------------------------------------------
# DeepSeek Configuration (provider: deepseek)
# Package: pip install langchain-deepseek
# --------------------------------------------------------------------------
DEEPSEEK_API_KEY=""
DEEPSEEK_MODEL="deepseek-chat"

# --------------------------------------------------------------------------
# Groq Configuration (provider: groq)
# Package: pip install langchain-groq
# --------------------------------------------------------------------------
GROQ_API_KEY=""
GROQ_MODEL="llama-3.1-8b-instant"

# --------------------------------------------------------------------------
# HuggingFace Configuration (provider: huggingface)
# Package: pip install langchain-huggingface
# --------------------------------------------------------------------------
HUGGINGFACE_API_KEY=""
HUGGINGFACE_REPO_ID="microsoft/Phi-3-mini-4k-instruct"

# --------------------------------------------------------------------------
# Ollama Configuration (provider: ollama)
# For running open-source models locally
# Package: pip install langchain-ollama
# Prerequisite: ollama pull <model_name>
# --------------------------------------------------------------------------
OLLAMA_MODEL="llama3"
OLLAMA_BASE_URL="http://localhost:11434"  # Default Ollama server URL

# --------------------------------------------------------------------------
# OpenAI-Compatible API Configuration (provider: openai_compat)
# For custom deployed models using OpenAI-compatible APIs
# (e.g., vLLM, TGI, LocalAI, LMStudio, etc.)
# Package: pip install langchain-openai
# --------------------------------------------------------------------------
OPENAI_COMPAT_API_KEY=""  # Your custom API key (or leave empty if not required)
OPENAI_COMPAT_MODEL=""    # Model name as expected by your deployment
OPENAI_COMPAT_BASE_URL="" # REQUIRED: Full URL to your OpenAI-compatible endpoint

# --------------------------------------------------------------------------
# Legacy/Fallback API Keys (for backward compatibility)
# --------------------------------------------------------------------------
AZURE_OPENAI_API_KEY=""
AZURE_OPENAI_ENDPOINT=""
AZURE_OPENAI_API_VERSION="2024-02-15-preview"
TOGETHER_API_KEY=""
DASHSCOPE_API_KEY=""

# Agent Workflow Configuration
AGENT_RECURSION_LIMIT=30          # Safety limit for maximum graph steps
AGENT_MAX_PLAN_ITERATIONS=1       # How many times the planner can revise its plan
AGENT_MAX_STEP_NUM=3              # Max steps execution per plan
AGENT_MAX_SEARCH_RESULTS=3        # Max results to fetch per search query
AGENT_ENABLE_DEEP_THINKING=False   # Enable usage of REASONING_MODEL if configured
AGENT_ENABLE_CLARIFICATION=False   # Allow agent to ask clarifying questions before starting
AGENT_MAX_CLARIFICATION_ROUNDS=3
AGENT_ENFORCE_WEB_SEARCH=False     # Force web search in every plan
AGENT_ENFORCE_RESEARCHER_SEARCH=True # Force researcher agent to use search tool

# --------------------------------------------------------------------------
# [Agent Middleware Configuration]
# Production-ready middleware for robust agent operation
# --------------------------------------------------------------------------

# Summarization Middleware - compresses long conversations to fit context windows
MIDDLEWARE_ENABLE_SUMMARIZATION=True
MIDDLEWARE_SUMMARIZATION_TRIGGER_TOKENS=100000  # Token count to trigger summarization
MIDDLEWARE_SUMMARIZATION_KEEP_MESSAGES=10       # Number of recent messages to preserve

# Model Retry Middleware - retries failed model calls with exponential backoff
MIDDLEWARE_ENABLE_MODEL_RETRY=True
MIDDLEWARE_MODEL_MAX_RETRIES=3
MIDDLEWARE_MODEL_BACKOFF_FACTOR=2.0
MIDDLEWARE_MODEL_INITIAL_DELAY=1.0

# Tool Retry Middleware - retries failed tool calls with exponential backoff
MIDDLEWARE_ENABLE_TOOL_RETRY=True
MIDDLEWARE_TOOL_MAX_RETRIES=3
MIDDLEWARE_TOOL_BACKOFF_FACTOR=2.0
MIDDLEWARE_TOOL_INITIAL_DELAY=0.5

# Model Call Limit Middleware - prevents runaway costs
MIDDLEWARE_ENABLE_MODEL_CALL_LIMIT=True
MIDDLEWARE_MODEL_CALL_THREAD_LIMIT=50   # Max model calls per thread
MIDDLEWARE_MODEL_CALL_RUN_LIMIT=25      # Max model calls per run

# Tool Call Limit Middleware - prevents excessive tool usage
MIDDLEWARE_ENABLE_TOOL_CALL_LIMIT=True
MIDDLEWARE_TOOL_CALL_THREAD_LIMIT=100   # Max tool calls per thread
MIDDLEWARE_TOOL_CALL_RUN_LIMIT=50       # Max tool calls per run

# Model Fallback Middleware - automatically fallback to alternative models when primary fails
MIDDLEWARE_ENABLE_MODEL_FALLBACK=True
# Comma-separated list of fallback model identifiers (e.g., "gpt-4o-mini,claude-3-5-sonnet")
# Uses provider:model format (e.g., "openai:gpt-4o-mini,anthropic:claude-3-5-sonnet")
# Or just model name to use same provider as primary (e.g., "gpt-4o-mini")
MIDDLEWARE_FALLBACK_MODELS="gpt-4o-mini"

# Fallback/Supplementary LLM Configuration
# Used for middleware operations like summarization when you want a different model
# Leave empty to use the primary LLM provider
FALLBACK_LLM_PROVIDER=""   # e.g., "openai", "anthropic", "groq"
FALLBACK_LLM_MODEL=""      # e.g., "gpt-4o-mini" for cost-effective summarization

# Tool Interrupts (Human-in-the-loop)
# Comma-separated list of tools to require approval for before execution
# TOOL_INTERRUPTS_BEFORE="db_write_tool,payment_api"

# --------------------------------------------------------------------------
# [Search & Crawler]
# --------------------------------------------------------------------------
# Search Engine Selection: tavily, infoquest, duckduckgo, brave, bing, searx
AGENT_SEARCH_ENGINE=tavily
TAVILY_API_KEY=tvly-xxx
INFOQUEST_API_KEY="infoquest-xxx"
BRAVE_SEARCH_API_KEY=""
BING_SEARCH_API_KEY=""
JINA_API_KEY="jina_xxx"
SEARX_HOST=""
EXA_API_KEY="" # For People Search tool (Exa.ai)
NCBI_API_KEY="" # For NCBI Search tool (PubMed)
SEMANTIC_SCHOLAR_API_KEY="" # For Paper Search tool (Semantic Scholar) 

# Detailed Search Config
SEARCH_ENGINE_INCLUDE_DOMAINS= []   # Limit search to specific domains (comma-separated)
SEARCH_ENGINE_EXCLUDE_DOMAINS= []      # Exclude specific domains
SEARCH_ENGINE_SEARCH_DEPTH="advanced" # "basic" or "advanced"
SEARCH_ENGINE_INCLUDE_ANSWER=False    # Include an AI-generated direct answer in results
SEARCH_ENGINE_INCLUDE_RAW_CONTENT=True # Fetch full page content
SEARCH_ENGINE_INCLUDE_IMAGES=True
SEARCH_ENGINE_INCLUDE_IMAGE_DESCRIPTIONS=True
SEARCH_ENGINE_MIN_SCORE_THRESHOLD=0.0
SEARCH_ENGINE_MAX_CONTENT_LENGTH=4000 # Max chars per page result

# InfoQuest Specific Settings
SEARCH_ENGINE_TIME_RANGE=30           # Limit search to last N days
SEARCH_ENGINE_SITE=""                 # Site filter

# Crawler Engine (jina or infoquest)
# Used to fetch detailed content from URLs found in search
CRAWLER_ENGINE=jina
CRAWLER_FETCH_TIME=10                 # Wait time after page load (seconds)
CRAWLER_TIMEOUT=30                    # Overall timeout (seconds)
CRAWLER_NAVI_TIMEOUT=15               # Navigation timeout (seconds)

# --------------------------------------------------------------------------
# [RAG Providers]
# --------------------------------------------------------------------------
# Selection: milvus, qdrant, vikingdb, ragflow, dify, moi
AGENT_RAG_PROVIDER=milvus

# Embeddings (Shared)
EMBEDDING_PROVIDER="openai" # openai, dashscope
EMBEDDING_BASE_URL=""
EMBEDDING_MODEL="text-embedding-ada-002"
EMBEDDING_API_KEY=""
AUTO_LOAD_EXAMPLES=True     # Auto-load RAG examples into DB on startup

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION="agent_documents"

# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=""
QDRANT_COLLECTION="agent_documents"

# RagFlow Configuration
RAGFLOW_API_URL="http://localhost:9388"
RAGFLOW_API_KEY="ragflow-xxx"
RAGFLOW_RETRIEVAL_SIZE=10

# Dify Configuration
DIFY_API_URL="https://api.dify.ai/v1"
DIFY_API_KEY="dataset-xxx"

# VikingDB Configuration
VIKINGDB_KNOWLEDGE_BASE_API_URL=""
VIKINGDB_KNOWLEDGE_BASE_API_AK=""
VIKINGDB_KNOWLEDGE_BASE_API_SK=""
VIKINGDB_KNOWLEDGE_BASE_RETRIEVAL_SIZE=15

# MatrixOne (MOI) Configuration
MOI_API_URL=""
MOI_API_KEY=""
MOI_RETRIEVAL_SIZE=10
MOI_LIST_LIMIT=10

# --------------------------------------------------------------------------
# [Persistence & Production]
# --------------------------------------------------------------------------
# LangGraph Checkpointer (Conversation Memory)
LANGGRAPH_CHECKPOINT_ENABLED=True
# Database Connection String for persistence (Postgres recommended for production)
LANGGRAPH_CHECKPOINT_DB_URL='postgresql://postgres:123456@127.0.0.1:5432/agents_backend'
# Set to 'True' in production to use Alembic migrations instead of auto-creating tables
AGENT_SKIP_DB_SETUP=False

# --------------------------------------------------------------------------
# [Other Services]
# --------------------------------------------------------------------------
# TTS (Text-to-Speech) - Volcengine
VOLCENGINE_TTS_APPID=''
VOLCENGINE_TTS_ACCESS_TOKEN=''
VOLCENGINE_TTS_CLUSTER='volcano_tts'
VOLCENGINE_TTS_VOICE_TYPE='BV700_V2_streaming'

# Agent API CORS
AGENT_ALLOWED_ORIGINS='http://localhost:3000'

# Agent Report Styles
AGENT_DEFAULT_REPORT_STYLE='ACADEMIC'


# --------------------------------------------------------------------------
# [Other Services]
# --------------------------------------------------------------------------
# --------------------------------------------------------------------------
# [Sandbox Configuration]
# --------------------------------------------------------------------------
SANDBOX_PROVIDER=e2b  # Options: e2b, daytona

# E2B Sandbox
E2B_API_KEY=your_e2b_api_key_here
E2B_TEMPLATE_ID=base

# Daytona Sandbox
DAYTONA_API_KEY=your_daytona_api_key_here
DAYTONA_SERVER_URL=https://app.daytona.io/api
DAYTONA_TARGET=us

# Agent-Infra Sandbox (Local Docker Sandbox)
# Used by DeepAgent CLI and LangChain Sandbox Tools
AGENT_INFRA_URL=http://localhost:8090
AGENT_INFRA_TIMEOUT=60

# Sandbox Service Ports (used by backend to connect to sandbox services)
SANDBOX_MCP_SERVER_PORT=6060
SANDBOX_CODE_SERVER_PORT=9000


# --------------------------------------------------------------------------
# [File Processing Configuration]
# Staged file upload and processing for chat attachments
# --------------------------------------------------------------------------

# Storage Backend: 'local' or 's3'
# - local: Store files on local filesystem (development/self-hosted)
# - s3: Store files in S3-compatible storage (production)
FILE_STORAGE_BACKEND=local

# Local Storage Configuration (when FILE_STORAGE_BACKEND=local)
FILE_STORAGE_LOCAL_PATH=./uploads/staged-files
FILE_STORAGE_LOCAL_BASE_URL=  # Optional: Base URL for serving files

# S3 Storage Configuration (when FILE_STORAGE_BACKEND=s3)
# Works with AWS S3, MinIO, Wasabi, DigitalOcean Spaces, etc.
FILE_STORAGE_S3_BUCKET=staged-files
FILE_STORAGE_S3_ENDPOINT_URL=  # Required for MinIO/Wasabi (e.g., http://localhost:9000)
FILE_STORAGE_S3_REGION=us-east-1
FILE_STORAGE_S3_ACCESS_KEY=
FILE_STORAGE_S3_SECRET_KEY=
FILE_STORAGE_S3_PUBLIC_URL_BASE=  # Optional: CDN/public URL for files

# File Size Limits
FILE_MAX_SIZE_MB=50  # Maximum file upload size in MB
FILE_MAX_PARSED_CONTENT_LENGTH=100000  # Max chars stored in database

# Staged File Expiration
FILE_STAGED_EXPIRY_HOURS=24  # Hours until staged files expire
FILE_SIGNED_URL_EXPIRY_SECONDS=3600  # Signed URL expiry (1 hour)

# Image Processing
FILE_IMAGE_MAX_WIDTH=2048   # Max width for image compression
FILE_IMAGE_MAX_HEIGHT=2048  # Max height for image compression
FILE_IMAGE_JPEG_QUALITY=85  # JPEG quality (1-100)

# Document Parsing Limits
FILE_PARSE_MAX_PDF_PAGES=500      # Max PDF pages to parse
FILE_PARSE_MAX_EXCEL_ROWS=100000  # Max Excel rows to parse
FILE_PARSE_MAX_TEXT_CHARS=10000000  # Max text chars (10M)


ALPHA_VANTAGE_API_KEY=your_alpha_vantage_api_key_here

# MCP Server Credentials
GITHUB_TOKEN=your_github_token_here
TAVILY_API_KEY=your_tavily_api_key_here

# LangSmith Tracing (Optional)
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=your_langsmith_project_here

# Langfuse Tracing (Optional)
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key_here
LANGFUSE_SECRET_KEY=your_langfuse_secret_key_here
LANGFUSE_HOST=your_langfuse_host_here


# =============================================================================
# Cloud Storage Credentials
# Provider is configured in config.yaml (storage.provider)
# =============================================================================

# -----------------------------------------------------------------------------
# AWS S3 Configuration
# -----------------------------------------------------------------------------
# AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
# S3_BUCKET_NAME=your_s3_bucket_name_here
# S3_REGION=us-east-1
# # Optional: CloudFront URL for faster delivery
# # S3_PUBLIC_URL_BASE=https://d1234.cloudfront.net

# -----------------------------------------------------------------------------
# Cloudflare R2
# -----------------------------------------------------------------------------
R2_ACCOUNT_ID=your_r2_account_id_here
R2_ACCESS_KEY_ID=your_r2_access_key_id_here
R2_SECRET_ACCESS_KEY=your_r2_secret_access_key_here
R2_BUCKET_NAME=your_r2_bucket_name_here
# Optional: Custom domain for public URLs (otherwise uses r2.dev)
# R2_PUBLIC_URL_BASE=https://cdn.yourdomain.com

# -----------------------------------------------------------------------------
# Alibaba Cloud OSS
# -----------------------------------------------------------------------------
# OSS_ACCESS_KEY_ID=your_oss_access_key_id_here
# OSS_ACCESS_KEY_SECRET=your_oss_access_key_secret_here
# OSS_BUCKET_NAME=your_oss_bucket_name_here
# OSS_REGION=oss-cn-hangzhou
# OSS_ENDPOINT=oss-cn-hangzhou.aliyuncs.com
# OSS_MAX_UPLOAD_SIZE=10485760



# --------------------------------------------------------------------------
# [Module] Billing & Stripe
# --------------------------------------------------------------------------
BILLING_ENABLED=True
STRIPE_SECRET_KEY=sk_test_replace_me
STRIPE_PUBLISHABLE_KEY=pk_test_replace_me
STRIPE_WEBHOOK_SECRET=whsec_replace_me

# Stripe Price IDs (Tiers)
STRIPE_FREE_TIER_ID=price_free_dummy
STRIPE_TIER_2_20_ID=price_tier_2_20_dummy
STRIPE_TIER_2_20_YEARLY_ID=price_tier_2_20_yearly_dummy
STRIPE_TIER_2_17_YEARLY_COMMITMENT_ID=price_tier_2_17_commitment_dummy
STRIPE_TIER_6_50_ID=price_tier_6_50_dummy
STRIPE_TIER_6_50_YEARLY_ID=price_tier_6_50_yearly_dummy
STRIPE_TIER_6_42_YEARLY_COMMITMENT_ID=price_tier_6_42_commitment_dummy
STRIPE_TIER_25_200_ID=price_tier_25_200_dummy
STRIPE_TIER_25_200_YEARLY_ID=price_tier_25_200_yearly_dummy
STRIPE_TIER_25_170_YEARLY_COMMITMENT_ID=price_tier_25_170_commitment_dummy

# Stripe Price IDs (Credit Packages)
STRIPE_CREDITS_10_PRICE_ID=price_credits_10_dummy
STRIPE_CREDITS_25_PRICE_ID=price_credits_25_dummy
STRIPE_CREDITS_50_PRICE_ID=price_credits_50_dummy
STRIPE_CREDITS_100_PRICE_ID=price_credits_100_dummy
STRIPE_CREDITS_250_PRICE_ID=price_credits_250_dummy
STRIPE_CREDITS_500_PRICE_ID=price_credits_500_dummy


# ============================================================================
# Tool Server Environment Variables
# ============================================================================
# This file lists all environment variables used by the tool_server module.
# Copy this file to `.env` and fill in the appropriate values.
# ============================================================================

# ----------------------------------------------------------------------------
# Core Configuration
# ----------------------------------------------------------------------------

# Required: Main database connection URL for the tool server
DATABASE_URL=

# Workspace directory for MCP server operations
WORKSPACE_DIR=

# Log file path for tool_server logging (default: /app/log/sandbox.log)
TOOL_SERVER_LOG_FILE=/app/log/sandbox.log

# ----------------------------------------------------------------------------
# OpenAI / LLM Configuration
# ----------------------------------------------------------------------------
# Used by LLMConfig (no prefix - uses lowercase field names directly)

OPENAI_API_KEY=
OPENAI_BASE_URL=http://localhost:4000
OPENAI_MODEL=gpt-5-mini

# Also used by EmbeddingCompressor (direct os.getenv call)
OPENAI_BASE_URL=http://localhost:4000

# Evaluation sleep time (default: 10 seconds) - used by backend/src/evals/evaluator.py
EVALUATION_SLEEP_TIME=10

# ----------------------------------------------------------------------------
# Web Visit Configuration (prefix: WEB_VISIT_)
# ----------------------------------------------------------------------------
# API keys for various web content extraction services

WEB_VISIT_FIRECRAWL_API_KEY=
WEB_VISIT_GEMINI_API_KEY=
WEB_VISIT_JINA_API_KEY=
WEB_VISIT_TAVILY_API_KEY=
WEB_VISIT_MAX_OUTPUT_LENGTH=40000

# ----------------------------------------------------------------------------
# Content Compressor Configuration (prefix: COMPRESSOR_)
# ----------------------------------------------------------------------------
# Settings for content compression using LLM or embeddings

# Comma-separated list: "llm", "embedding", or both "llm,embedding"
COMPRESSOR_COMPRESS_TYPES=["llm"]
# JSON object for embedding configuration
COMPRESSOR_EMBEDDING_CONFIG=
# JSON object for LLM configuration
COMPRESSOR_LLM_CONFIG=
COMPRESSOR_MAX_OUTPUT_WORDS=6500
COMPRESSOR_MAX_INPUT_WORDS=32000
COMPRESSOR_CHUNK_SIZE=1000
COMPRESSOR_CHUNK_OVERLAP=0
COMPRESSOR_SIMILARITY_THRESHOLD=0.3

# ----------------------------------------------------------------------------
# Web Search Configuration (prefix: WEB_SEARCH_)
# ----------------------------------------------------------------------------
# API keys for web search services

WEB_SEARCH_FIRECRAWL_API_KEY=
WEB_SEARCH_SERPAPI_API_KEY=
WEB_SEARCH_JINA_API_KEY=
WEB_SEARCH_TAVILY_API_KEY=
WEB_SEARCH_MAX_RESULTS=5

# ----------------------------------------------------------------------------
# Image Search Configuration (prefix: IMAGE_SEARCH_)
# ----------------------------------------------------------------------------

IMAGE_SEARCH_SERPAPI_API_KEY=
IMAGE_SEARCH_MAX_RESULTS=5

# ----------------------------------------------------------------------------
# Image Generation Configuration (prefix: IMAGE_GENERATE_)
# ----------------------------------------------------------------------------
# Google Cloud Platform settings for image generation

IMAGE_GENERATE_GCP_PROJECT_ID=
IMAGE_GENERATE_GCP_LOCATION=
IMAGE_GENERATE_GCS_OUTPUT_BUCKET=
IMAGE_GENERATE_GOOGLE_AI_STUDIO_API_KEY=

# ----------------------------------------------------------------------------
# Video Generation Configuration (prefix: VIDEO_GENERATE_)
# ----------------------------------------------------------------------------
# Google Cloud Platform settings for video generation

VIDEO_GENERATE_GCP_PROJECT_ID=
VIDEO_GENERATE_GCP_LOCATION=
VIDEO_GENERATE_GCS_OUTPUT_BUCKET=
VIDEO_GENERATE_GOOGLE_AI_STUDIO_API_KEY=

# ----------------------------------------------------------------------------
# Database Configuration (prefix: DATABASE_)
# ----------------------------------------------------------------------------

# NeonDB API key for PostgreSQL provisioning
DATABASE_NEON_DB_API_KEY=

# Direct database connection URLs (used by factory.py)
REDIS_URL=
MYSQL_URL=

# ----------------------------------------------------------------------------
# Storage Configuration
# ----------------------------------------------------------------------------
# Google Cloud Storage settings (no prefix - uses lowercase field names)

storage_provider=gcs
gcs_bucket_name=
gcs_project_id=
