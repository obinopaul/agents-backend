# Env
# Recommended for development and troubleshooting
# DEBUG=True
# APP_ENV=development
ENVIRONMENT='dev'

# Database
DATABASE_TYPE='postgresql'
DATABASE_HOST='127.0.0.1'
DATABASE_PORT=5432
DATABASE_USER='postgres'
DATABASE_PASSWORD='123456'

# Redis
REDIS_HOST='127.0.0.1'
REDIS_PORT=6379
REDIS_PASSWORD=''
REDIS_USERNAME='default'  # For Redis 6+ ACL (cloud Redis). Use 'default' if no specific user.
REDIS_DATABASE=0

# Token
TOKEN_SECRET_KEY='1VkVF75nsNABBjK_7-qz7GtzNy3AMvktc9TCPwKczCk'
# Opera Log
OPERA_LOG_ENCRYPT_SECRET_KEY='d77b25790a804c2b4a339dd0207941e4cefa5751935a33735bc73bb7071a005b'
# [ App ] task
# Celery
CELERY_BROKER_REDIS_DATABASE=1
# Rabbitmq
CELERY_RABBITMQ_HOST='127.0.0.1'
CELERY_RABBITMQ_PORT=5672
CELERY_RABBITMQ_USERNAME='guest'
CELERY_RABBITMQ_PASSWORD='guest'
# [ Plugin ] oauth2
OAUTH2_GITHUB_CLIENT_ID='test'
OAUTH2_GITHUB_CLIENT_SECRET='test'
OAUTH2_GOOGLE_CLIENT_ID='test'
OAUTH2_GOOGLE_CLIENT_SECRET='test'
OAUTH2_LINUX_DO_CLIENT_ID='test'
OAUTH2_LINUX_DO_CLIENT_SECRET='test'
# [ Plugin ] email
EMAIL_USERNAME=''
EMAIL_PASSWORD=''

# ============================================================
# [ Module ] Agent - LangChain/LangGraph AI Agents
# ============================================================

# --------------------------------------------------------------------------
# [Robust Agent Configuration]
# --------------------------------------------------------------------------

# Basic Model (LLM) - The primary model for chat and planning
# Supports OpenAI-compatible endpoints (Doubao, OpenAI, vLLM, etc.)
BASIC_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
BASIC_MODEL_NAME="doubao-1-5-pro-32k-250115"
BASIC_MODEL_API_KEY="xxxx"
BASIC_MODEL_MAX_RETRIES=3
BASIC_MODEL_TOKEN_LIMIT=200000 # Max input tokens for context compression (prevents overflow)
BASIC_MODEL_VERIFY_SSL=True    # Set to False if using self-signed certs

# Reasoning Model (Optional)
# Specialized model for complex "Deep Thinking" tasks (e.g., DeepSeek R1, Doubao Thinking)
REASONING_MODEL_BASE_URL="https://ark.cn-beijing.volces.com/api/v3"
REASONING_MODEL_NAME="doubao-1-5-thinking-pro-m-250428"
REASONING_MODEL_API_KEY="xxxx"
REASONING_MODEL_MAX_RETRIES=3
REASONING_MODEL_TOKEN_LIMIT=150000

# Provider API Keys (Fallbacks)
# These are used if you don't configure BASIC_MODEL, or for specific provider features
OPENAI_API_KEY=''
OPENAI_API_BASE=''
ANTHROPIC_API_KEY=''
AZURE_OPENAI_API_KEY=''
AZURE_OPENAI_ENDPOINT=''
AZURE_OPENAI_API_VERSION='2024-02-15-preview'
GOOGLE_API_KEY=''
TOGETHER_API_KEY=''
DEEPSEEK_API_KEY=''
DASHSCOPE_API_KEY=''

# Agent Workflow Configuration
AGENT_RECURSION_LIMIT=30          # Safety limit for maximum graph steps
AGENT_MAX_PLAN_ITERATIONS=1       # How many times the planner can revise its plan
AGENT_MAX_STEP_NUM=3              # Max steps execution per plan
AGENT_MAX_SEARCH_RESULTS=3        # Max results to fetch per search query
AGENT_ENABLE_DEEP_THINKING=False   # Enable usage of REASONING_MODEL if configured
AGENT_ENABLE_CLARIFICATION=False   # Allow agent to ask clarifying questions before starting
AGENT_MAX_CLARIFICATION_ROUNDS=3
AGENT_ENFORCE_WEB_SEARCH=False     # Force web search in every plan
AGENT_ENFORCE_RESEARCHER_SEARCH=True # Force researcher agent to use search tool

# Python REPL Sandbox (WARNING: Security Risk)
# Enable only in trusted environments (e.g., Docker container). Allows generating charts/math.
ENABLE_PYTHON_REPL=False

# Tool Interrupts (Human-in-the-loop)
# Comma-separated list of tools to require approval for before execution
# TOOL_INTERRUPTS_BEFORE="db_write_tool,payment_api"

# --------------------------------------------------------------------------
# [Search & Crawler]
# --------------------------------------------------------------------------
# Search Engine Selection: tavily, infoquest, duckduckgo, brave, bing, searx
AGENT_SEARCH_ENGINE=tavily
TAVILY_API_KEY=tvly-xxx
INFOQUEST_API_KEY="infoquest-xxx"
BRAVE_SEARCH_API_KEY=""
BING_SEARCH_API_KEY=""
JINA_API_KEY="jina_xxx"
SEARX_HOST=""

# Detailed Search Config
SEARCH_ENGINE_INCLUDE_DOMAINS= []   # Limit search to specific domains (comma-separated)
SEARCH_ENGINE_EXCLUDE_DOMAINS= []      # Exclude specific domains
SEARCH_ENGINE_SEARCH_DEPTH="advanced" # "basic" or "advanced"
SEARCH_ENGINE_INCLUDE_ANSWER=False    # Include an AI-generated direct answer in results
SEARCH_ENGINE_INCLUDE_RAW_CONTENT=True # Fetch full page content
SEARCH_ENGINE_INCLUDE_IMAGES=True
SEARCH_ENGINE_INCLUDE_IMAGE_DESCRIPTIONS=True
SEARCH_ENGINE_MIN_SCORE_THRESHOLD=0.0
SEARCH_ENGINE_MAX_CONTENT_LENGTH=4000 # Max chars per page result

# InfoQuest Specific Settings
SEARCH_ENGINE_TIME_RANGE=30           # Limit search to last N days
SEARCH_ENGINE_SITE=""                 # Site filter

# Crawler Engine (jina or infoquest)
# Used to fetch detailed content from URLs found in search
CRAWLER_ENGINE=jina
CRAWLER_FETCH_TIME=10                 # Wait time after page load (seconds)
CRAWLER_TIMEOUT=30                    # Overall timeout (seconds)
CRAWLER_NAVI_TIMEOUT=15               # Navigation timeout (seconds)

# --------------------------------------------------------------------------
# [RAG Providers]
# --------------------------------------------------------------------------
# Selection: milvus, qdrant, vikingdb, ragflow, dify, moi
AGENT_RAG_PROVIDER=milvus

# Embeddings (Shared)
EMBEDDING_PROVIDER="openai" # openai, dashscope
EMBEDDING_BASE_URL=""
EMBEDDING_MODEL="text-embedding-ada-002"
EMBEDDING_API_KEY=""
AUTO_LOAD_EXAMPLES=True     # Auto-load RAG examples into DB on startup

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION="agent_documents"

# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=""
QDRANT_COLLECTION="agent_documents"

# RagFlow Configuration
RAGFLOW_API_URL="http://localhost:9388"
RAGFLOW_API_KEY="ragflow-xxx"
RAGFLOW_RETRIEVAL_SIZE=10

# Dify Configuration
DIFY_API_URL="https://api.dify.ai/v1"
DIFY_API_KEY="dataset-xxx"

# VikingDB Configuration
VIKINGDB_KNOWLEDGE_BASE_API_URL=""
VIKINGDB_KNOWLEDGE_BASE_API_AK=""
VIKINGDB_KNOWLEDGE_BASE_API_SK=""
VIKINGDB_KNOWLEDGE_BASE_RETRIEVAL_SIZE=15

# MatrixOne (MOI) Configuration
MOI_API_URL=""
MOI_API_KEY=""
MOI_RETRIEVAL_SIZE=10
MOI_LIST_LIMIT=10

# --------------------------------------------------------------------------
# [Persistence & Production]
# --------------------------------------------------------------------------
# LangGraph Checkpointer (Conversation Memory)
LANGGRAPH_CHECKPOINT_ENABLED=True
# Database Connection String for persistence (Postgres recommended for production)
LANGGRAPH_CHECKPOINT_DB_URL='postgresql://postgres:123456@127.0.0.1:5432/fba'
# Set to 'True' in production to use Alembic migrations instead of auto-creating tables
AGENT_SKIP_DB_SETUP=False

# --------------------------------------------------------------------------
# [Other Services]
# --------------------------------------------------------------------------
# TTS (Text-to-Speech) - Volcengine
VOLCENGINE_TTS_APPID=''
VOLCENGINE_TTS_ACCESS_TOKEN=''
VOLCENGINE_TTS_CLUSTER='volcano_tts'
VOLCENGINE_TTS_VOICE_TYPE='BV700_V2_streaming'

# Agent API CORS
AGENT_ALLOWED_ORIGINS='http://localhost:3000'

# Agent Report Styles
AGENT_DEFAULT_REPORT_STYLE='ACADEMIC'


# --------------------------------------------------------------------------
# [Other Services]
# --------------------------------------------------------------------------
# --------------------------------------------------------------------------
# [Sandbox Configuration]
# --------------------------------------------------------------------------
SANDBOX_PROVIDER=e2b  # Options: e2b, daytona

# E2B Sandbox
E2B_API_KEY=your_e2b_api_key_here
E2B_TEMPLATE_ID=base

# Daytona Sandbox
DAYTONA_API_KEY=your_daytona_api_key_here
DAYTONA_SERVER_URL=https://app.daytona.io/api
DAYTONA_TARGET=us

# Sandbox Service Ports (used by backend to connect to sandbox services)
SANDBOX_MCP_SERVER_PORT=6060
SANDBOX_CODE_SERVER_PORT=9000

ALPHA_VANTAGE_API_KEY=your_alpha_vantage_api_key_here

# MCP Server Credentials
GITHUB_TOKEN=your_github_token_here
TAVILY_API_KEY=your_tavily_api_key_here

# LangSmith Tracing (Optional)
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=your_langsmith_project_here


# =============================================================================
# Cloud Storage Credentials
# Provider is configured in config.yaml (storage.provider)
# =============================================================================

# -----------------------------------------------------------------------------
# AWS S3 Configuration
# -----------------------------------------------------------------------------
# AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
# S3_BUCKET_NAME=your_s3_bucket_name_here
# S3_REGION=us-east-1
# # Optional: CloudFront URL for faster delivery
# # S3_PUBLIC_URL_BASE=https://d1234.cloudfront.net

# -----------------------------------------------------------------------------
# Cloudflare R2
# -----------------------------------------------------------------------------
R2_ACCOUNT_ID=your_r2_account_id_here
R2_ACCESS_KEY_ID=your_r2_access_key_id_here
R2_SECRET_ACCESS_KEY=your_r2_secret_access_key_here
R2_BUCKET_NAME=your_r2_bucket_name_here
# Optional: Custom domain for public URLs (otherwise uses r2.dev)
# R2_PUBLIC_URL_BASE=https://cdn.yourdomain.com

# -----------------------------------------------------------------------------
# Alibaba Cloud OSS
# -----------------------------------------------------------------------------
# OSS_ACCESS_KEY_ID=your_oss_access_key_id_here
# OSS_ACCESS_KEY_SECRET=your_oss_access_key_secret_here
# OSS_BUCKET_NAME=your_oss_bucket_name_here
# OSS_REGION=oss-cn-hangzhou
# OSS_ENDPOINT=oss-cn-hangzhou.aliyuncs.com
# OSS_MAX_UPLOAD_SIZE=10485760



# ============================================================================
# Tool Server Environment Variables
# ============================================================================
# This file lists all environment variables used by the tool_server module.
# Copy this file to `.env` and fill in the appropriate values.
# ============================================================================

# ----------------------------------------------------------------------------
# Core Configuration
# ----------------------------------------------------------------------------

# Required: Main database connection URL for the tool server
DATABASE_URL=

# Workspace directory for MCP server operations
WORKSPACE_DIR=

# Log file path for tool_server logging (default: /app/log/sandbox.log)
TOOL_SERVER_LOG_FILE=/app/log/sandbox.log

# ----------------------------------------------------------------------------
# OpenAI / LLM Configuration
# ----------------------------------------------------------------------------
# Used by LLMConfig (no prefix - uses lowercase field names directly)

openai_api_key=
openai_base_url=http://localhost:4000
openai_model=gpt-5-mini

# Also used by EmbeddingCompressor (direct os.getenv call)
OPENAI_BASE_URL=http://localhost:4000

# ----------------------------------------------------------------------------
# Web Visit Configuration (prefix: WEB_VISIT_)
# ----------------------------------------------------------------------------
# API keys for various web content extraction services

WEB_VISIT_FIRECRAWL_API_KEY=
WEB_VISIT_GEMINI_API_KEY=
WEB_VISIT_JINA_API_KEY=
WEB_VISIT_TAVILY_API_KEY=
WEB_VISIT_MAX_OUTPUT_LENGTH=40000

# ----------------------------------------------------------------------------
# Content Compressor Configuration (prefix: COMPRESSOR_)
# ----------------------------------------------------------------------------
# Settings for content compression using LLM or embeddings

# Comma-separated list: "llm", "embedding", or both "llm,embedding"
COMPRESSOR_COMPRESS_TYPES=["llm"]
# JSON object for embedding configuration
COMPRESSOR_EMBEDDING_CONFIG=
# JSON object for LLM configuration
COMPRESSOR_LLM_CONFIG=
COMPRESSOR_MAX_OUTPUT_WORDS=6500
COMPRESSOR_MAX_INPUT_WORDS=32000
COMPRESSOR_CHUNK_SIZE=1000
COMPRESSOR_CHUNK_OVERLAP=0
COMPRESSOR_SIMILARITY_THRESHOLD=0.3

# ----------------------------------------------------------------------------
# Web Search Configuration (prefix: WEB_SEARCH_)
# ----------------------------------------------------------------------------
# API keys for web search services

WEB_SEARCH_FIRECRAWL_API_KEY=
WEB_SEARCH_SERPAPI_API_KEY=
WEB_SEARCH_JINA_API_KEY=
WEB_SEARCH_TAVILY_API_KEY=
WEB_SEARCH_MAX_RESULTS=5

# ----------------------------------------------------------------------------
# Image Search Configuration (prefix: IMAGE_SEARCH_)
# ----------------------------------------------------------------------------

IMAGE_SEARCH_SERPAPI_API_KEY=
IMAGE_SEARCH_MAX_RESULTS=5

# ----------------------------------------------------------------------------
# Image Generation Configuration (prefix: IMAGE_GENERATE_)
# ----------------------------------------------------------------------------
# Google Cloud Platform settings for image generation

IMAGE_GENERATE_GCP_PROJECT_ID=
IMAGE_GENERATE_GCP_LOCATION=
IMAGE_GENERATE_GCS_OUTPUT_BUCKET=
IMAGE_GENERATE_GOOGLE_AI_STUDIO_API_KEY=

# ----------------------------------------------------------------------------
# Video Generation Configuration (prefix: VIDEO_GENERATE_)
# ----------------------------------------------------------------------------
# Google Cloud Platform settings for video generation

VIDEO_GENERATE_GCP_PROJECT_ID=
VIDEO_GENERATE_GCP_LOCATION=
VIDEO_GENERATE_GCS_OUTPUT_BUCKET=
VIDEO_GENERATE_GOOGLE_AI_STUDIO_API_KEY=

# ----------------------------------------------------------------------------
# Database Configuration (prefix: DATABASE_)
# ----------------------------------------------------------------------------

# NeonDB API key for PostgreSQL provisioning
DATABASE_NEON_DB_API_KEY=

# Direct database connection URLs (used by factory.py)
REDIS_URL=
MYSQL_URL=

# ----------------------------------------------------------------------------
# Storage Configuration
# ----------------------------------------------------------------------------
# Google Cloud Storage settings (no prefix - uses lowercase field names)

storage_provider=gcs
gcs_bucket_name=
gcs_project_id=
